{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb82b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T09:09:58.259132Z",
     "start_time": "2025-10-19T09:08:21.033672Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U litellm langchain_ollama langchain_community langchain_anthropic langchain-tavily langchain_experimental matplotlib langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3108cb5725fc6d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:16.411142Z",
     "start_time": "2025-10-19T15:45:52.620481Z"
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c529c44a82b3ba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:17.489632Z",
     "start_time": "2025-10-19T15:46:16.419124Z"
    }
   },
   "outputs": [],
   "source": [
    "llm_gen_qwen3_coder = ChatOpenAI(\n",
    "    model=\"qwen3-coder:30b\",  # Указанная модель\n",
    "    openai_api_base=\"https://cloud.m1r0.ru/v1\",\n",
    "    openai_api_key=\"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\",\n",
    "    temperature=0\n",
    ")\n",
    "print(f\"llm_gen_qwen3_coder инициализирован для модели: {llm_gen_qwen3_coder.model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f492bed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:21.474098Z",
     "start_time": "2025-10-19T15:46:21.467461Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./Датасет/flights.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print(data.keys(), data['queries'][0].keys())\n",
    "\n",
    "DDL_querys = \"\\n\".join([i['statement'] for i in data['ddl']])\n",
    "SQL_querys = [i['query'] for i in data['queries']]\n",
    "\n",
    "defolt_retry_nums = 3\n",
    "MAX_CONCURRENT_CALLS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594c4fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:23.998553Z",
     "start_time": "2025-10-19T15:46:23.994004Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_analize_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Упрощённый промпт для query_analize_agent.\n",
    "    Агент анализирует до 5 SQL-запросов и возвращает текстовое описание закономерностей,\n",
    "    часто используемых таблиц, соединений, фильтров и потенциальных точек оптимизации под DDL.\n",
    "    \"\"\"\n",
    "    return \"\"\"Ты — SQL-аналитик для системы оптимизации Data Lakehouse (Trino + Iceberg + S3).\n",
    "Вход: до 5 SQL-запросов.  \n",
    "Твоя цель — описать словами структуру и частые паттерны этих запросов, указав:\n",
    "- какие таблицы чаще всего участвуют;\n",
    "- какие поля часто используются в фильтрах (WHERE/ON);\n",
    "- по каким полям часто происходит соединение (JOIN);\n",
    "- какие операции чаще всего выполняются (GROUP BY, ORDER BY, DISTINCT, WINDOW);\n",
    "- какие конструкции создают нагрузку или могут быть оптимизированы через DDL;\n",
    "- какие улучшения можно предложить на уровне DDL (партиционирование, денормализация, сортировка и т. п.).\n",
    "\n",
    "### Важно:\n",
    "0. Обращай внимание на частоту выполнения запроса и его время, чтобы в первую очередь оптимизировать тяжёлые запросы\n",
    "1. **Запрещено** предлагать или использовать materialized views.\n",
    "2. **Запрещено** менять DDL или писать SQL-код миграций. Только описывать, что требует оптимизации.  \n",
    "3. **Запрещено** придумывать статистику, размеры таблиц или время выполнения. Если данных нет — укажи \"неизвестно\".  \n",
    "4. **Не упоминай** индексы в классическом RDBMS-смысле (B-Tree и т. д.).  \n",
    "5. **Не упоминай** безопасность, авторизацию, шифрование и внешние системы.  \n",
    "6. Используй формулировки, применимые к Trino + Iceberg + S3.  \n",
    "   (Например: «можно рассмотреть партиционирование по дате» вместо «создать индекс».)  \n",
    "7. Не возвращай JSON, таблицы или списки ключей. Пиши связный текстовый отчёт с пунктами и примерами.\n",
    "\n",
    "### Структура ответа:\n",
    "- Краткое резюме: какие таблицы и поля чаще всего встречаются.  \n",
    "- Анализ соединений: какие таблицы чаще соединяются между собой, по каким ключам.  \n",
    "- Анализ фильтров: какие колонки часто участвуют в WHERE/ON (например, event_date, user_id).  \n",
    "- Анализ операций: какие операции создают нагрузку (JOIN, GROUP BY, DISTINCT, ORDER BY, WINDOW).\n",
    "\n",
    "Вывод должен быть понятным, логически структурированным и готовым для последующей обработки другой моделью, которая будет строить DDL и миграции на основе твоего анализа.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6cb0df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:24.165129Z",
     "start_time": "2025-10-19T15:46:24.159866Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_summarize_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Возвращает промпт (на русском) для query_summarize_agent.\n",
    "    Агент получает агрегированные результаты от query_analize_agent (строка/батч)\n",
    "    и должен выдать компактный, читаемый человеком и машиной, сводный отчёт\n",
    "    — пригодный для подачи в DDL_agent и migrations_agent.\n",
    "    \"\"\"\n",
    "\n",
    "    return \"\"\"Ты — агрегирующий аналитик SQL для пайплайна оптимизации Data Lakehouse (стек: Trino + Iceberg + S3).\n",
    "\n",
    "ВХОД:\n",
    "- Строка / текст `analize` — объединённые результаты работы query_analize_agent для многих батчей (каждый блок содержит анализ до 5 SQL-запросов).\n",
    "- Формат входа может быть JSON-подобным или текстовым отчётом (см. предыдущий агент). Разбирай оба варианта; если не можешь распарсить фрагмент — помечь как \"UNPARSED fragment\".\n",
    "\n",
    "ЦЕЛЬ:\n",
    "- Объединить и агрегировать мелкие анализы в единый сводный отчёт, выявив:\n",
    "  1. \"Hot\" таблицы — таблицы с наибольшей частотой упоминаний в тяжёлых операциях;\n",
    "  2. Частые пары JOIN (и их join_keys);\n",
    "  3. Часто используемые фильтры (колонки в WHERE/ON);\n",
    "  4. Частые тяжёлые операции (GROUP BY, ORDER BY, DISTINCT, WINDOW, SORT);\n",
    "  5. Повторяющиеся паттерны, которые можно исправить DDL-изменениями;\n",
    "  6. Очередность (приоритет) изменений для downstream: DDL_agent -> migrations_agent -> query_optimizer.\n",
    "- Сформировать короткий и однозначный набор рекомендаций уровня DDL (НЕ писать сам DDL/миграции, а только что и где изменить и почему).\n",
    "\n",
    "СТРОГИЕ ЗАПРЕЩЕНИЯ (выполняй обязательно):\n",
    "- НЕЛЬЗЯ предлагать или использовать materialized views.\n",
    "- НЕЛЬЗЯ писать, изменять или генерировать DDL или миграции — это делают downstream агенты.\n",
    "- НЕЛЬЗЯ придумывать статистику, объёмы данных, кардинальности или время выполнения; если этих метрик нет — указывай `UNKNOWN`.\n",
    "- НЕЛЬЗЯ предлагать классические RDBMS-индексы (B-Tree и т.п.). Если предлагаешь что-то похожее, опиши его как \"файловая/партиц./сортировка/кластеризация в Iceberg\" и пометь `depends_on_iceberg_features`.\n",
    "- НЕЛЬЗЯ обсуждать авторизацию/аутентификацию/безопасность/внешние сервисы.\n",
    "- Ответ должен быть ТОЛЬКО текстом отчёта (без генерации JSON/DDL/SQL). Пиши структурированный человекочитаемый отчёт (см. формат ниже).\n",
    "\n",
    "ПРАВИЛА АГРЕГАЦИИ:\n",
    "- Подсчитывай частоту встречаемости сущностей (таблиц, колонок, join-пар) по данным `analize`. Если входы не содержат явного счётчика — используй относительную частоту (high/medium/low) основанную на количестве вхождений в анализах; если нельзя определить — ставь `UNKNOWN`.\n",
    "- Если множество анализов указывает на фильтрацию по колонке `event_date`/`ds` — предлагай PARTITIONING (указывай день/месяц как опции). Добавляй замечание о риске при высокой кардинальности.\n",
    "- Если одна и та же пара таблиц соединяется очень часто — размышляй о DENORMALIZATION (опиши, какие поля включить), но НЕ генерируй CREATE TABLE.\n",
    "- Для ORDER BY + LIMIT — указывай возможность SORT_ORDER / предварительной кластеризации в Iceberg и помечай `depends_on_iceberg_features`.\n",
    "- В каждой рекомендации указывай: цель (что менять), конкретные колонки/таблицы, краткая причина (основанную на evidence fragments из `analize`), ориентировочный impact (HIGH/MEDIUM/LOW/UNKNOWN) и какие метрики/проверки нужны (data_volume, cardinality, query_frequency, avg_runtime).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f88a366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:24.350149Z",
     "start_time": "2025-10-19T15:46:24.342409Z"
    }
   },
   "outputs": [],
   "source": [
    "def ddl_optim_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Промпт для агента ddl_optimize_agent.\n",
    "    Цель — на основе анализа (query_summarize) и оригинальных DDL (ddl_orig)\n",
    "    выдать только новые или модифицированные DDL-запросы, оптимизирующие работу под Trino + Iceberg + S3.\n",
    "    Без пояснений, текста и комментариев.\n",
    "    \"\"\"\n",
    "    return \"\"\"Ты — DDL-оптимизатор для Trino + Iceberg + S3.\n",
    "\n",
    "Вход:\n",
    "- query_summarize: анализ частых фильтров, сортировок, JOIN\n",
    "- ddl_orig: исходные DDL таблиц\n",
    "\n",
    "Задача: Создать оптимизированные DDL для Trino + Iceberg.\n",
    "\n",
    "СИНТАКСИС Trino + Iceberg:\n",
    "- Таблицы: <catalog>.<schema>.<table>\n",
    "- Формат хранения файлов: используйте WITH (format = 'PARQUET')\n",
    "- Партиционирование (Trino/Iceberg): в CREATE TABLE через WITH (partitioning = ARRAY['day(ts_col)'] или 'month(ts_col)'). Не комбинировать year/month/day на одном столбце.\n",
    "- Свойства: только допустимые Trino/Iceberg ключи в WITH (...). Не использовать 'write.target-file-size-bytes'. Без висячих запятых.\n",
    "\n",
    "РАЗРЕШЕНО:\n",
    "- PARTITIONING (через WITH) с допустимыми функциями\n",
    "- - CREATE TABLE AS SELECT WITH (format = 'PARQUET')\n",
    "- Создание оптимизированных копий\n",
    "\n",
    "ЗАПРЕЩЕНО:\n",
    "- Materialized Views, индексы\n",
    "- DROP, DELETE, RENAME\n",
    "- Несовместимый синтаксис\n",
    "\n",
    "ПРИМЕРЫ:\n",
    "ALTER TABLE analytics.sales.orders PARTITIONING (через WITH) year(order_date);\n",
    "CREATE TABLE analytics.sales.orders_new AS SELECT * FROM orders WITH (format = 'PARQUET');\n",
    "ALTER TABLE analytics.sales.orders -- removed unsafe SET PROPERTIES example\n",
    "\n",
    "ПРАВИЛА ВЫВОДА:\n",
    "- Каждый оператор — отдельной строкой; не объединяй множество операторов в одну строку.\n",
    "- Полные имена таблиц (<catalog>.<schema>.<table>) согласованы с JDBC url.\n",
    "- Никаких комментариев, префиксов типа \"sql \". Только чистые SQL-операторы.\n",
    "\n",
    "ПРАВИЛА ВЫВОДА:\n",
    "- Каждый оператор — отдельной строкой; не объединяй множество операторов в одну строку.\n",
    "- Полные имена таблиц (<catalog>.<schema>.<table>) согласованы с JDBC url.\n",
    "- Никаких комментариев, префиксов типа \"sql \". Только чистые SQL-операторы.\n",
    "\n",
    "ПРАВИЛА ВЫВОДА:\n",
    "- Каждый оператор — отдельной строкой; не объединяй множество операторов в одну строку.\n",
    "- Полные имена таблиц (<catalog>.<schema>.<table>) согласованы с JDBC url.\n",
    "- Никаких комментариев, префиксов типа \"sql \". Только чистые SQL-операторы.\n",
    "\n",
    "ВЫВОД: Только SQL DDL команды, готовые к выполнению в Trino.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9534a30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:24.501385Z",
     "start_time": "2025-10-19T15:46:24.497467Z"
    }
   },
   "outputs": [],
   "source": [
    "def migrations_creator_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Промпт для агента migrations_creator_agent.\n",
    "    Агент получает:\n",
    "      - query_summarize — сводный анализ SQL-запросов,\n",
    "      - new_ddl — новые DDL-запросы, сгенерированные ddl_optim_agent.\n",
    "    Его задача — сгенерировать SQL-миграции для применения новых DDL в безопасном формате,\n",
    "    совместимом с Trino + Iceberg + S3.\n",
    "    \"\"\"\n",
    "    return \"\"\"Ты — генератор миграций для Trino + Iceberg + S3.\n",
    "\n",
    "ВХОД:\n",
    "- query_summarize: анализ использования таблиц\n",
    "- new_ddl: оптимизированные DDL-запросы\n",
    "\n",
    "ЦЕЛЬ:\n",
    "Сгенерировать безопасные SQL-миграции для применения DDL в Trino + Iceberg.\n",
    "\n",
    "КРИТИЧЕСКИ ВАЖНЫЕ ПРАВИЛА СИНТАКСИСА:\n",
    "1. ВСЕ таблицы: <catalog>.<schema>.<table>\n",
    "2. ТОЛЬКО совместимый с Trino + Iceberg синтаксис\n",
    "3. НЕТ materialized views, индексов, деструктивных операций\n",
    "4. Формат файлов: используем 'PARQUET' через WITH (format = 'PARQUET')\n",
    "\n",
    "РАЗРЕШЕННЫЕ ОПЕРАЦИИ МИГРАЦИИ:\n",
    "- Не дублировать DDL из new_ddl: миграции не должны повторять CREATE TABLE/CTAS, уже присутствующие в new_ddl.\n",
    "- CREATE TABLE ... WITH (format = 'PARQUET', partitioning = ARRAY[...])\n",
    "- ALTER TABLE ... PARTITIONING (через WITH) ... (только допустимые функции: year/month/day/hour/bucket/truncate)\n",
    "- INSERT INTO new_table SELECT FROM old_table\n",
    "- CREATE TABLE ... AS SELECT ... WITH (format = 'PARQUET')\n",
    "\n",
    "ПОРЯДОК МИГРАЦИЙ:\n",
    "1. Создание новых таблиц с оптимизациями\n",
    "2. Перенос данных (если нужно)\n",
    "3. Изменение свойств существующих таблиц\n",
    "4. Валидация (простая проверка COUNT)\n",
    "\n",
    "ПРИМЕРЫ ВАЛИДНОГО СИНТАКСИСА:\n",
    "CREATE TABLE analytics.sales.orders_new\n",
    "WITH (format = 'PARQUET', partitioning = ARRAY['day(order_date)'])\n",
    "AS SELECT * FROM analytics.sales.orders;\n",
    "\n",
    "ALTER TABLE analytics.sales.orders\n",
    "PARTITIONING (через WITH) year(order_date);\n",
    "\n",
    "ALTER TABLE analytics.sales.orders\n",
    "-- removed unsafe SET PROPERTIES example\n",
    "\n",
    "ВЫВОД:\n",
    "ТОЛЬКО SQL-команды миграций, готовые к выполнению в Trino.\n",
    "Без комментариев, пояснений, текста.\n",
    "Каждая команда на новой строке.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f9ed3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:24.671202Z",
     "start_time": "2025-10-19T15:46:24.666328Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_optimize_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Промпт для агента query_optimizer.\n",
    "    Агент получает новые DDL-запросы (new_ddl) и один исходный SQL-запрос (query),\n",
    "    и должен переписать данный запрос так, чтобы он был оптимизирован под новую структуру данных\n",
    "    (DDL), сохраняя исходную бизнес-логику и корректность результатов.\n",
    "    Работает в контексте Trino + Iceberg + S3.\n",
    "    \"\"\"\n",
    "    return \"\"\"Ты — SQL-оптимизатор для Trino + Iceberg + S3.\n",
    "\n",
    "ВХОД:\n",
    "- new_ddl: новые DDL таблиц (партиции, денормализации, свойства)\n",
    "- query: исходный SQL запрос для оптимизации\n",
    "\n",
    "ЦЕЛЬ:\n",
    "Переписать SQL запрос для использования новой структуры таблиц из DDL.\n",
    "Сохранить идентичную бизнес-логику и результат.\n",
    "\n",
    "ПРАВИЛА ОПТИМИЗАЦИИ:\n",
    "1. Используй новые таблицы и колонки из DDL\n",
    "2. Для денормализованных таблиц - убирай JOIN, используй прямые обращения\n",
    "3. Для партиционированных таблиц - фильтруй по полям партиций\n",
    "4. Сохраняй все агрегации, фильтры и логику оригинала\n",
    "5. Используй только синтаксис Trino + Iceberg\n",
    "\n",
    "ЗАПРЕЩЕНО:\n",
    "- Менять бизнес-логику (WHERE, JOIN, GROUP BY кроме адаптации к DDL)\n",
    "- Materialized Views, индексы, временные таблицы\n",
    "- Комментарии, пояснения, не-SQL текст\n",
    "\n",
    "ПРИМЕР:\n",
    "Исходный: SELECT u.id, SUM(o.amount) FROM orders o JOIN users u ON o.user_id = u.id\n",
    "DDL: CREATE TABLE orders_denorm AS SELECT o.*, u.region FROM orders o JOIN users u ON o.user_id = u.id\n",
    "Оптимизированный: SELECT user_id, SUM(amount) FROM orders_denorm\n",
    "\n",
    "АНТИ-ОШИБКИ (соблюдать обязательно):\n",
    "- Каждый источник в FROM/JOIN обязан иметь алиас; все колонки должны быть квалифицированы этим алиасом.\n",
    "- Нельзя ссылаться на алиас другой таблицы (пример: pc.client_id, если в FROM pc= l_excursion_payment, где нет client_id). Выбирай корректные поля исходной таблицы или корректируй JOIN.\n",
    "- Любая подзапрос/CTE обязан иметь алиас (.. ) AS sub; не оставлять безымянные скобки.\n",
    "- GROUP BY: перечисляй только реально присутствующие в SELECT выражения/поля; не добавлять лишние токены (например, 'b1').\n",
    "- Скобки и запятые: не оставлять висячих запятых и лишних закрывающих скобок.\n",
    "- ORDER BY random() использовать только если это требуется задачей; избегать недетерминизма.\n",
    "\n",
    "ВЫВОД: Только оптимизированный SQL запрос для Trino, без комментариев.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509797ce7388692",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:24.835170Z",
     "start_time": "2025-10-19T15:46:24.828624Z"
    }
   },
   "outputs": [],
   "source": [
    "def critic_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Prompt for LLM‑critic: given original/optimized SQL and other data, return a fixed SQL.\n",
    "    \"\"\"\n",
    "    return \"\"\"Ты — LLM‑критик SQL для Trino + Iceberg.\n",
    "\n",
    "ВХОД (plain‑text блоки, без JSON):\n",
    "\n",
    "old_ddl:\n",
    "<CREATE TABLE ...>  (по одному на строку; опционально)\n",
    "\n",
    "new_ddl:\n",
    "<CREATE TABLE ...>  (по одному на строку; опционально)\n",
    "\n",
    "migrations:\n",
    "<CTAS/INSERT INTO ... SELECT ...>  (перенос из старой структуры в новую; опционально)\n",
    "\n",
    "original:\n",
    "<исходный SQL>\n",
    "\n",
    "optimized:\n",
    "<оптимизированный SQL>\n",
    "\n",
    "ЦЕЛЬ:\n",
    "- Самостоятельно найти и исправить ошибки в поле optimized, если они конечно присутствуют. Проведи собственную многошаговую проверку консистентности, используя при наличии DDL (ddl_old/ddl_new) как источник истины по колонкам и таблицам.\n",
    "- Исходный запрос (original) практически всегда валиден. Разрешено переиспользовать его конструкции (алиасы, выражения, подзапросы, группировки) во втором запросе, но обязательно с учётом уже выполненной оптимизации (например, денормализация, замена источников, добавление партиционных фильтров).\n",
    "- Проверь строго по чек‑листу:\n",
    "  1) Алиасы: каждый источник в FROM/JOIN имеет алиас; все колонки квалифицированы правильным алиасом.\n",
    "  2) Существование колонок: alias.column существует согласно DDL соответствующей таблицы; при денормализации используй новые таблицы из ddl_new. Если DDL отсутствуют — делай правки только по очевидным несоответствиям (алиасы/скобки/Group By), не придумывай колонки.\n",
    "  3) JOIN‑ключи: ссылки только на колонки, реально присутствующие в соответствующих таблицах; не используй client_id у таблицы, где его нет.\n",
    "  4) Агрегации: GROUP BY соответствует SELECT (все неагрегированные выражения перечислены), нет мусорных токенов (например, 'b1').\n",
    "  5) Синтаксис: нет лишних/незакрытых скобок, висячих запятых.\n",
    "  6) Детерминизм: избегай ORDER BY random() без явной необходимости.\n",
    "- Если оптимизация меняла структуру: корректно перепиши обращения к колонкам и JOIN с учётом ddl_new.\n",
    "- Если ошибок НЕТ — верни строку \"OK\" (без дополнительных слов).\n",
    "- Если ошибки ЕСТЬ — верни ТОЛЬКО один исправленный SQL‑запрос, без комментариев и текста.\n",
    "\n",
    "ОБЯЗАТЕЛЬНО:\n",
    "- Каждый источник в FROM/JOIN должен иметь алиас.\n",
    "- Все колонки должны быть квалифицированы правильным алиасом.\n",
    "- Не ссылаться на поля, которых нет в источнике (пример: pc.client_id, если pc= l_excursion_payment, где нет client_id).\n",
    "- Каждый подзапрос/CTE обязан иметь алиас.\n",
    "- GROUP BY: перечисляй только выражения/поля из SELECT; не добавляй посторонние токены.\n",
    "- Скобки и запятые: без висячих запятых и лишних скобок.\n",
    "- Использовать синтаксис Trino; таблицы — в формате <catalog>.<schema>.<table>.\n",
    "- Избегать ORDER BY random(), если это не требуется явно.\n",
    "\n",
    "ВЫВОД:\n",
    "- Если ошибок нет: ровно \"OK\".\n",
    "- Если есть ошибки: только исправленный SQL.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40047fc7535e3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:46:25.013185Z",
     "start_time": "2025-10-19T15:46:25.008447Z"
    }
   },
   "outputs": [],
   "source": [
    "def judge_prompt() -> str:\n",
    "    \"\"\"\n",
    "    Prompt for LLM‑judge: decide if optimized preserves original semantics from structure only.\n",
    "    \"\"\"\n",
    "    return \"\"\"Ты — LLM‑судья эквивалентности SQL (Trino).\n",
    "\n",
    "ВХОД (plain‑text блоки, без JSON):\n",
    "\n",
    "old_ddl:\n",
    "<CREATE TABLE ...>  (по одному на строку; опционально)\n",
    "\n",
    "new_ddl:\n",
    "<CREATE TABLE ...>  (по одному на строку; опционально)\n",
    "\n",
    "migrations:\n",
    "<CTAS/INSERT INTO ... SELECT ...>  (перенос из старой в новую; опционально)\n",
    "\n",
    "original:\n",
    "<исходный SQL>\n",
    "\n",
    "optimized:\n",
    "<оптимизированный SQL>\n",
    "\n",
    "ЗАДАЧА:\n",
    "- По структуре SQL (без выполнения) оцени, сохраняет ли optimized бизнес‑логику original (те же агрегаты, фильтры, соединения, проекции) с допустимыми адаптациями под оптимизацию (денормализация, партиционные фильтры и т. п.).\n",
    "- Учитывай DDL/миграции: если optimized читает данные из новых таблиц (по CTAS/INSERT), интерпретируй соответствия полей между старой и новой структурой.\n",
    "\n",
    "ВЫВОД:\n",
    "- Если ошибок нет: ровно \"OK\".\n",
    "- Если есть проблема (семантика/структура/синтаксис): верни полный исправленный SQL (один запрос, без комментариев/объяснений).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ca3ac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:48:06.751174Z",
     "start_time": "2025-10-19T15:46:28.774907Z"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def get_answer_async(agent, querys_batch, retry_nums=3):\n",
    "    for _ in range(retry_nums):\n",
    "        try:\n",
    "            prompt = HumanMessage(\n",
    "                content='\\n\\n'.join(querys_batch)\n",
    "            )\n",
    "            response = await agent.ainvoke({'messages': prompt})\n",
    "            return response[\"messages\"][-1].content\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to execute. Error: {repr(e)}\")\n",
    "            print(\"Retrying...\")\n",
    "    print(\"OMG_MODEL_CRUSHD!!!\")\n",
    "    return \"OMG_MODEL_CRUSHD!!!\"\n",
    "\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_CALLS)\n",
    "\n",
    "async def bounded_get_answer_async(agent, batch):\n",
    "    async with semaphore:\n",
    "        return await get_answer_async(agent, batch)\n",
    "\n",
    "query_analize_agent = create_react_agent(\n",
    "    llm_gen_qwen3_coder,\n",
    "    tools=[],\n",
    "    prompt=query_analize_prompt(),\n",
    ")\n",
    "\n",
    "querys_batch_prompt = [f\"sql-запрос: {i['query']}, количество выполнения:{i['runquantity']}, колличество затраченного времени при едином выполнении запроса{i['executiontime']}\" for i in data['queries']]\n",
    "\n",
    "batches = [\n",
    "    querys_batch_prompt[5 * i : 5 * (i + 1)]\n",
    "    for i in range(len(querys_batch_prompt) // 5 + int(len(querys_batch_prompt) % 5 != 0))\n",
    "]\n",
    "\n",
    "tasks = [bounded_get_answer_async(query_analize_agent, batch) for batch in batches]\n",
    "\n",
    "results = await asyncio.gather(*tasks)\n",
    "\n",
    "analize = \"\\n\\n\".join(results)\n",
    "print(analize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea92be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:48:38.467012Z",
     "start_time": "2025-10-19T15:48:06.829524Z"
    }
   },
   "outputs": [],
   "source": [
    "# summaraize analize from querys\n",
    "\n",
    "def get_answer(agent, querys_analize):\n",
    "    for _ in range(defolt_retry_nums):\n",
    "        try:\n",
    "            prompt = HumanMessage(\n",
    "                content= f\"Анализ полученных sql запросов{querys_analize}\"\n",
    "            )\n",
    "            return agent.invoke({'messages': prompt})[\"messages\"][-1].content\n",
    "        except BaseException as e:\n",
    "            print(f\"Failed to execute. Error: {repr(e)}\")\n",
    "            print(\"Do retry\")\n",
    "    print(\"OMG_MODEL_CRUSHD!!!\")\n",
    "    \n",
    "    return \"OMG_MODEL_CRUSHD!!!\"\n",
    "\n",
    "query_summarize_agent = create_react_agent(\n",
    "    llm_gen_qwen3_coder,\n",
    "    tools=[],\n",
    "    prompt=query_summarize_prompt(),\n",
    ")\n",
    "\n",
    "query_summarize = get_answer(query_summarize_agent, analize)\n",
    "print(query_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a590a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:48:41.487952Z",
     "start_time": "2025-10-19T15:48:38.478128Z"
    }
   },
   "outputs": [],
   "source": [
    "# DDL optimize by analize and orig_ddl\n",
    "\n",
    "def get_answer(agent, query_summarize, ddl_orig):\n",
    "    for _ in range(defolt_retry_nums):\n",
    "        try:\n",
    "            prompt = HumanMessage(\n",
    "                content= f\"Анализ полученных sql запросов:\\n{query_summarize} \\n\\n это оригинальные DDL запросы: \\n{ddl_orig}\"\n",
    "            )\n",
    "            return agent.invoke({'messages': prompt})[\"messages\"][-1].content\n",
    "        except BaseException as e:\n",
    "            print(f\"Failed to execute. Error: {repr(e)}\")\n",
    "            print(\"Do retry\")\n",
    "    print(\"OMG_MODEL_CRUSHD!!!\")\n",
    "    return \"OMG_MODEL_CRUSHD!!!\"\n",
    "\n",
    "ddl_optimize_agent = create_react_agent(\n",
    "    llm_gen_qwen3_coder,\n",
    "    tools=[],\n",
    "    prompt=ddl_optim_prompt(),\n",
    ")\n",
    "\n",
    "New_DDL = get_answer(ddl_optimize_agent, query_summarize, DDL_querys)\n",
    "print(New_DDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b1431a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:48:44.875204Z",
     "start_time": "2025-10-19T15:48:41.495102Z"
    }
   },
   "outputs": [],
   "source": [
    "# create migrations on analize and new_ddl\n",
    "\n",
    "def get_answer(agent, query_summarize, new_ddl):\n",
    "    for _ in range(defolt_retry_nums):\n",
    "        try:\n",
    "            prompt = HumanMessage(\n",
    "                content= f\"Анализ полученных sql запросов:\\n{query_summarize} \\n\\n это Новые DDL запросы: \\n{new_ddl}\"\n",
    "            )\n",
    "            return agent.invoke({'messages': prompt})[\"messages\"][-1].content\n",
    "        except BaseException as e:\n",
    "            print(f\"Failed to execute. Error: {repr(e)}\")\n",
    "            print(\"Do retry\")\n",
    "    print(\"OMG_MODEL_CRUSHD!!!\")\n",
    "    return \"OMG_MODEL_CRUSHD!!!\"\n",
    "\n",
    "migrations_creator_agent = create_react_agent(\n",
    "    llm_gen_qwen3_coder,\n",
    "    tools=[],\n",
    "    prompt=migrations_creator_prompt(),\n",
    ")\n",
    "\n",
    "migrations = get_answer(migrations_creator_agent, query_summarize, New_DDL)\n",
    "print(migrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5556475098643b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:48:44.908555Z",
     "start_time": "2025-10-19T15:48:44.898387Z"
    }
   },
   "outputs": [],
   "source": [
    "codect_info={\n",
    "        'old_ddl':DDL_querys, 'new_ddl':New_DDL, 'migrations': migrations\n",
    "    }\n",
    "\n",
    "judge_agent = create_react_agent(\n",
    "    llm_gen_qwen3_coder,\n",
    "    tools=[],\n",
    "    prompt=judge_prompt(),\n",
    ")\n",
    "critic_agent = create_react_agent(\n",
    "    llm_gen_qwen3_coder,\n",
    "    tools=[],\n",
    "    prompt=critic_prompt(),\n",
    ")\n",
    "\n",
    "global codect_info, judge_agent, critic_agent\n",
    "\n",
    "\n",
    "async def cheak_query(old_querys, new_querys):\n",
    "    global codect_info, judge_agent, critic_agent\n",
    "    prompt = HumanMessage(\n",
    "        content=(\n",
    "            f\"старые DDL запросы: \\n{codect_info['old_ddl']}\\n\\n\"\n",
    "            f\"Новые DDL запросы: \\n{codect_info['new_ddl']}\\n\\n\"\n",
    "            f\"Вот миграции:\\n{codect_info['migrations']}\"\n",
    "            f\"оригинальный sql запрос:\\n{old_querys}\"\n",
    "            f\"оптимизированный запрос:\\n{new_querys}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    judge_response = await judge_agent.ainvoke({'messages': prompt})\n",
    "    cheak_logik = judge_response[\"messages\"][-1].content\n",
    "\n",
    "    if not 'OK' in cheak_logik:\n",
    "        new_querys = cheak_logik\n",
    "\n",
    "    prompt = HumanMessage(\n",
    "        content=(\n",
    "            f\"старые DDL запросы: \\n{codect_info['old_ddl']}\\n\\n\"\n",
    "            f\"Новые DDL запросы: \\n{codect_info['new_ddl']}\\n\\n\"\n",
    "            f\"Вот миграции:\\n{codect_info['migrations']}\"\n",
    "            f\"оригинальный sql запрос:\\n{old_querys}\"\n",
    "            f\"оптимизированный запрос:\\n{new_querys}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    critic_response = await critic_agent.ainvoke({'messages': prompt})\n",
    "    cheak_sintax = critic_response[\"messages\"][-1].content\n",
    "\n",
    "    if not 'OK' in cheak_sintax:\n",
    "        new_querys = cheak_sintax\n",
    "\n",
    "    return new_querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410d79505fde58fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:52:25.609959Z",
     "start_time": "2025-10-19T15:48:44.925123Z"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "semaphore = asyncio.Semaphore(MAX_CONCURRENT_CALLS)\n",
    "\n",
    "async def get_answer_async(agent, new_ddl, query, retry_nums=3):\n",
    "    for _ in range(retry_nums):\n",
    "        try:\n",
    "            prompt = HumanMessage(\n",
    "                content=(\n",
    "                    f\"Новые DDL запросы: \\n{new_ddl}\\n\\n\"\n",
    "                    f\"Вот SQL запрос, который надо оптимизировать:\\n{query}\"\n",
    "                )\n",
    "            )\n",
    "            response = await agent.ainvoke({'messages': prompt})\n",
    "            return await cheak_query(query, response[\"messages\"][-1].content)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to execute for query: {query}. Error: {repr(e)}\")\n",
    "            print(\"Retrying...\")\n",
    "            await asyncio.sleep(1)\n",
    "    print(f\"OMG_MODEL_CRUSHD!!! for query: {query}\")\n",
    "    return \"OMG_MODEL_CRUSHD!!!\"\n",
    "\n",
    "async def get_answer_with_semaphore(agent, new_ddl, query, semaphore):\n",
    "    async with semaphore:\n",
    "        return await get_answer_async(agent, new_ddl, query)\n",
    "\n",
    "query_optimize_agent = create_react_agent(\n",
    "    llm_gen_qwen3_coder,\n",
    "    tools=[],\n",
    "    prompt=query_optimize_prompt(),\n",
    ")\n",
    "\n",
    "tasks = [\n",
    "    get_answer_with_semaphore(query_optimize_agent, New_DDL, query, semaphore)\n",
    "    for query in SQL_querys\n",
    "]\n",
    "results = await asyncio.gather(*tasks)\n",
    "optim_querys = {\n",
    "    data[\"queries\"][i][\"queryid\"]: results[i]\n",
    "    for i in range(len(results))\n",
    "}\n",
    "optim_querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2365ca22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:55:40.622445Z",
     "start_time": "2025-10-19T15:55:40.617192Z"
    }
   },
   "outputs": [],
   "source": [
    "json_answer = {\n",
    "    'ddl':data['ddl'] + [\n",
    "        {'statement': i} for i in New_DDL.replace('\\n', ' ').split(';')[:-1]\n",
    "    ],\n",
    "    'migrations':[\n",
    "        {'statement': i} for i in migrations.replace('\\n', ' ').split(';')[:-1]\n",
    "    ],\n",
    "    'queries':[\n",
    "        {'queryid': i['queryid'],\n",
    "         'query': optim_querys[i['queryid']].replace('\\n', ' ')} for i in data['queries']\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab72e3c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T15:56:00.996742Z",
     "start_time": "2025-10-19T15:56:00.991917Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('flights_GigaTest_1_2.json', 'w') as fp:\n",
    "    json.dump(json_answer, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d73b980ad31233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e418e413c6a858b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d36ab6f1752ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132a5f55d1f04c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7b9bb73fb4d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
